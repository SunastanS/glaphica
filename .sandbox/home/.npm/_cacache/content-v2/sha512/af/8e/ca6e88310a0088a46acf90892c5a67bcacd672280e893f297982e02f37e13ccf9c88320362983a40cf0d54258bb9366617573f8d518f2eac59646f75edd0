{"_id":"@ruvector/graph-transformer","name":"@ruvector/graph-transformer","dist-tags":{"latest":"2.0.4"},"versions":{"2.0.4":{"name":"@ruvector/graph-transformer","version":"2.0.4","description":"Proof-gated graph transformer with 8 verified modules — physics, biological, manifold, temporal, economic graph intelligence via NAPI-RS","main":"index.js","types":"index.d.ts","napi":{"name":"ruvector-graph-transformer","triples":{"defaults":false,"additional":["x86_64-unknown-linux-gnu","x86_64-unknown-linux-musl","aarch64-unknown-linux-gnu","aarch64-unknown-linux-musl","x86_64-apple-darwin","aarch64-apple-darwin","x86_64-pc-windows-msvc"]}},"scripts":{"artifacts":"napi artifacts","build":"napi build --platform --release","build:debug":"napi build --platform","prepublishOnly":"napi prepublish -t npm","test":"cargo test -p ruvector-graph-transformer-node","version":"napi version"},"keywords":["ruvector","graph-transformer","proof-gated","attention","verified-training","gnn","graph-neural-network","napi-rs"],"author":{"name":"Ruvector Team"},"license":"MIT","repository":{"type":"git","url":"git+https://github.com/ruvnet/ruvector.git"},"devDependencies":{"@napi-rs/cli":"^2.16.0"},"engines":{"node":">= 10"},"publishConfig":{"registry":"https://registry.npmjs.org/","access":"public"},"optionalDependencies":{"@ruvector/graph-transformer-linux-x64-gnu":"2.0.4","@ruvector/graph-transformer-linux-x64-musl":"2.0.4","@ruvector/graph-transformer-linux-arm64-gnu":"2.0.4","@ruvector/graph-transformer-linux-arm64-musl":"2.0.4","@ruvector/graph-transformer-darwin-x64":"2.0.4","@ruvector/graph-transformer-darwin-arm64":"2.0.4","@ruvector/graph-transformer-win32-x64-msvc":"2.0.4"},"_id":"@ruvector/graph-transformer@2.0.4","gitHead":"95c256a4c4e53a0cb44afa3aab22bc50dc328ad0","bugs":{"url":"https://github.com/ruvnet/ruvector/issues"},"homepage":"https://github.com/ruvnet/ruvector#readme","_nodeVersion":"22.21.1","_npmVersion":"9.8.1","dist":{"integrity":"sha512-Ep7nCq4vwJ41CR/yl+isYXZAxi1qOLoDIJPVrM//zDx9aixRm4n1TWu9PSsv7GSXizSusQwv+n9hUeZrJ2muTg==","shasum":"f2b77f09fcc7c1712f6ca730731079e938e13c67","tarball":"https://registry.npmjs.org/@ruvector/graph-transformer/-/graph-transformer-2.0.4.tgz","fileCount":13,"unpackedSize":1574696,"signatures":[{"keyid":"SHA256:DhQ8wR5APBvFHLF/+Tc+AYvPOdTpcIDqOhxsBHRwC7U","sig":"MEYCIQCa6y1+os/+C32IXyx584k6YmzOvgo9oZwYGvGCR4wp+QIhANuATwSGRlz0v0oqac++zClPax3XXU3H5Z+nqlpEZmsV"}]},"_npmUser":{"name":"ruvnet","email":"ruv@ruv.net"},"directories":{},"maintainers":[{"name":"ruvnet","email":"ruv@ruv.net"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages-npm-production","tmp":"tmp/graph-transformer_2.0.4_1772026369891_0.3311612042923966"},"_hasShrinkwrap":false}},"time":{"created":"2026-02-25T13:32:49.826Z","2.0.4":"2026-02-25T13:32:50.147Z","modified":"2026-02-25T13:32:50.325Z"},"maintainers":[{"name":"ruvnet","email":"ruv@ruv.net"}],"description":"Proof-gated graph transformer with 8 verified modules — physics, biological, manifold, temporal, economic graph intelligence via NAPI-RS","homepage":"https://github.com/ruvnet/ruvector#readme","keywords":["ruvector","graph-transformer","proof-gated","attention","verified-training","gnn","graph-neural-network","napi-rs"],"repository":{"type":"git","url":"git+https://github.com/ruvnet/ruvector.git"},"author":{"name":"Ruvector Team"},"bugs":{"url":"https://github.com/ruvnet/ruvector/issues"},"license":"MIT","readme":"# @ruvector/graph-transformer\n\n[![npm](https://img.shields.io/npm/v/@ruvector/graph-transformer.svg)](https://www.npmjs.com/package/@ruvector/graph-transformer)\n[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](LICENSE)\n[![Tests](https://img.shields.io/badge/tests-20_passing-brightgreen.svg)]()\n\n**Node.js bindings for RuVector Graph Transformer — proof-gated graph attention, verified training, and 8 specialized graph layers via NAPI-RS.**\n\nUse graph transformers from JavaScript and TypeScript with native Rust performance. Every graph operation — adding nodes, computing attention, training weights — produces a formal proof receipt proving it was done correctly. The heavy computation runs in compiled Rust via NAPI-RS, so you get sub-millisecond proof verification without leaving the Node.js ecosystem.\n\n## Install\n\n```bash\nnpm install @ruvector/graph-transformer\n```\n\nPrebuilt binaries are provided for:\n\n| Platform | Architecture | Package |\n|----------|-------------|---------|\n| Linux | x64 (glibc) | `@ruvector/graph-transformer-linux-x64-gnu` |\n| Linux | x64 (musl) | `@ruvector/graph-transformer-linux-x64-musl` |\n| Linux | ARM64 (glibc) | `@ruvector/graph-transformer-linux-arm64-gnu` |\n| macOS | x64 (Intel) | `@ruvector/graph-transformer-darwin-x64` |\n| macOS | ARM64 (Apple Silicon) | `@ruvector/graph-transformer-darwin-arm64` |\n| Windows | x64 | `@ruvector/graph-transformer-win32-x64-msvc` |\n\n## Quick Start\n\n```javascript\nconst { GraphTransformer } = require('@ruvector/graph-transformer');\n\nconst gt = new GraphTransformer();\nconsole.log(gt.version()); // \"2.0.4\"\n\n// Proof-gated mutation\nconst gate = gt.createProofGate(128);\nconsole.log(gate.dimension); // 128\n\n// Prove dimension equality\nconst proof = gt.proveDimension(128, 128);\nconsole.log(proof.verified); // true\n\n// Create attestation (82-byte proof receipt)\nconst attestation = gt.createAttestation(proof.proof_id);\nconsole.log(attestation.length); // 82\n```\n\n## API Reference\n\n### Proof-Gated Operations\n\n```javascript\n// Create a proof gate for a dimension\nconst gate = gt.createProofGate(dim);\n\n// Prove two dimensions are equal\nconst proof = gt.proveDimension(expected, actual);\n\n// Create 82-byte attestation for embedding in RVF witness chains\nconst bytes = gt.createAttestation(proofId);\n\n// Verify attestation from bytes\nconst valid = gt.verifyAttestation(bytes);\n\n// Compose a pipeline of type-checked stages\nconst composed = gt.composeProofs([\n  { name: 'embed', input_type_id: 1, output_type_id: 2 },\n  { name: 'align', input_type_id: 2, output_type_id: 3 },\n]);\n```\n\n### Sublinear Attention\n\n```javascript\n// O(n log n) graph attention via PPR sparsification\nconst result = gt.sublinearAttention(\n  [1.0, 0.5, -0.3],     // query vector\n  [[1, 2], [0, 2], [0, 1]], // adjacency list\n  3,                      // dimension\n  2                       // top-k\n);\nconsole.log(result.top_k_indices, result.sparsity_ratio);\n\n// Raw PPR scores\nconst scores = gt.pprScores(0, [[1], [0, 2], [1]], 0.15);\n```\n\n### Physics-Informed Layers\n\n```javascript\n// Symplectic leapfrog step (energy-conserving)\nconst state = gt.hamiltonianStep([1.0, 0.0], [0.0, 1.0], 0.01);\nconsole.log(state.energy);\n\n// With graph interactions\nconst state2 = gt.hamiltonianStepGraph(\n  [1.0, 0.0], [0.0, 1.0],\n  [{ src: 0, tgt: 1 }], 0.01\n);\nconsole.log(state2.energy_conserved); // true\n```\n\n### Biological Layers\n\n```javascript\n// Spiking neural attention (event-driven)\nconst output = gt.spikingAttention(\n  [0.5, 1.5, 0.3],          // membrane potentials\n  [[1], [0, 2], [1]],       // adjacency\n  1.0                        // firing threshold\n);\n\n// Hebbian weight update (Hebb's rule)\nconst weights = gt.hebbianUpdate(\n  [1.0, 0.0],  // pre-synaptic\n  [0.0, 1.0],  // post-synaptic\n  [0, 0, 0, 0], // current weights (flattened)\n  0.1            // learning rate\n);\n\n// Full spiking step over feature matrix\nconst result = gt.spikingStep(\n  [[0.8, 0.6], [0.1, 0.2]],  // n x dim features\n  [0, 0.5, 0.3, 0]            // flat adjacency (n x n)\n);\n```\n\n### Verified Training\n\n```javascript\n// Single verified SGD step with proof receipt\nconst result = gt.verifiedStep(\n  [1.0, 2.0],  // weights\n  [0.1, 0.2],  // gradients\n  0.01          // learning rate\n);\nconsole.log(result.proof_id, result.loss_before, result.loss_after);\n\n// Full training step with features and targets\nconst step = gt.verifiedTrainingStep(\n  [1.0, 2.0],   // features\n  [0.5, 1.0],   // targets\n  [0.5, 0.5]    // weights\n);\nconsole.log(step.certificate_id, step.loss);\n```\n\n### Manifold Operations\n\n```javascript\n// Product manifold distance (mixed curvatures)\nconst d = gt.productManifoldDistance(\n  [1, 0, 0, 1],    // point a\n  [0, 1, 1, 0],    // point b\n  [0.0, -1.0]      // curvatures (Euclidean, Hyperbolic)\n);\n\n// Product manifold attention\nconst result = gt.productManifoldAttention(\n  [1.0, 0.5, -0.3, 0.8],\n  [{ src: 0, tgt: 1 }]\n);\n```\n\n### Temporal-Causal Attention\n\n```javascript\n// Causal attention (no future information leakage)\nconst scores = gt.causalAttention(\n  [1.0, 0.0],                        // query\n  [[1.0, 0.0], [0.0, 1.0], [0.5, 0.5]], // keys\n  [1.0, 2.0, 3.0]                    // timestamps\n);\n\n// Causal attention over graph\nconst output = gt.causalAttentionGraph(\n  [1.0, 0.5, 0.8],    // node features\n  [1.0, 2.0, 3.0],    // timestamps\n  [{ src: 0, tgt: 1 }, { src: 1, tgt: 2 }]\n);\n\n// Granger causality extraction\nconst dag = gt.grangerExtract(flatHistory, 3, 20);\nconsole.log(dag.edges); // [{ source, target, f_statistic, is_causal }]\n```\n\n### Economic / Game-Theoretic\n\n```javascript\n// Nash equilibrium attention\nconst result = gt.gameTheoreticAttention(\n  [1.0, 0.5, 0.8],  // utility values\n  [{ src: 0, tgt: 1 }, { src: 1, tgt: 2 }]\n);\nconsole.log(result.allocations, result.nash_gap, result.converged);\n```\n\n### Stats & Control\n\n```javascript\n// Aggregate statistics\nconst stats = gt.stats();\nconsole.log(stats.proofs_verified, stats.attestations_created);\n\n// Reset all internal state\ngt.reset();\n```\n\n## Building from Source\n\n```bash\n# Install NAPI-RS CLI\nnpm install -g @napi-rs/cli\n\n# Build native module\ncd crates/ruvector-graph-transformer-node\nnapi build --platform --release\n\n# Run tests\ncargo test -p ruvector-graph-transformer-node\n```\n\n## Related Packages\n\n| Package | Description |\n|---------|-------------|\n| [`ruvector-graph-transformer`](../ruvector-graph-transformer) | Core Rust crate |\n| [`ruvector-graph-transformer-wasm`](../ruvector-graph-transformer-wasm) | WASM bindings for browsers |\n| [`@ruvector/gnn`](https://www.npmjs.com/package/@ruvector/gnn) | Base GNN operations |\n| [`@ruvector/attention`](https://www.npmjs.com/package/@ruvector/attention) | 46 attention mechanisms |\n\n## License\n\nMIT\n","readmeFilename":"README.md","_rev":"1-32493918ffc2450b2fbfb104eeb3d9d8"}
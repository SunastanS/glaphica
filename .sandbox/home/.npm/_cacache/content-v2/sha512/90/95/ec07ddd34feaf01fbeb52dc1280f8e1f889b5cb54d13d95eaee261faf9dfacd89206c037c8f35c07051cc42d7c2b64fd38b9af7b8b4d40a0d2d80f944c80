{"_id":"ruvector-attention-wasm","_rev":"2-1c863d2ef93b80dea98e1efff6fbd8dc","name":"ruvector-attention-wasm","dist-tags":{"latest":"0.1.32"},"versions":{"0.1.0":{"name":"ruvector-attention-wasm","version":"0.1.0","keywords":["wasm","webassembly","attention","transformer","machine-learning","neural-networks","hyperbolic","moe","flash-attention","graph-attention","rust"],"author":{"name":"rUv","email":"ruv@ruv.io"},"license":"MIT OR Apache-2.0","_id":"ruvector-attention-wasm@0.1.0","maintainers":[{"name":"ruvnet","email":"ruv@ruv.net"}],"homepage":"https://github.com/ruvnet/ruvector","bugs":{"url":"https://github.com/ruvnet/ruvector/issues"},"dist":{"shasum":"f17436c3b46dd99b8c9091946ea46872e66ba1c0","tarball":"https://registry.npmjs.org/ruvector-attention-wasm/-/ruvector-attention-wasm-0.1.0.tgz","fileCount":7,"integrity":"sha512-kYdKs5fH2LkUz2TmBbSjN3m/0ZtmaOihiyPeDYDq8bwHTc3bCVxAw3bPZoY/OQvsDy34uhE/EDnqMxnpU4TWoA==","signatures":[{"sig":"MEUCIQCbv9CnG0LIZoiVs3BKU5PJ/T3q67FzV59IQWE2N/qW1QIgWv4n3raDaMoc7a73LyJF5cZuM04bgAy0k043hG/m778=","keyid":"SHA256:DhQ8wR5APBvFHLF/+Tc+AYvPOdTpcIDqOhxsBHRwC7U"}],"unpackedSize":236549},"main":"ruvector_attention_wasm.js","type":"module","types":"ruvector_attention_wasm.d.ts","gitHead":"8a61930d00a6d84b50a039aa479c8041fdf8949a","_npmUser":{"name":"ruvnet","email":"ruv@ruv.net"},"repository":{"url":"git+https://github.com/ruvnet/ruvector.git","type":"git"},"_npmVersion":"9.8.1","description":"High-performance attention mechanisms for WebAssembly - Transformer, Hyperbolic, Flash, MoE, and Graph attention","directories":{},"sideEffects":["./snippets/*"],"_nodeVersion":"22.21.1","_hasShrinkwrap":false,"_npmOperationalInternal":{"tmp":"tmp/ruvector-attention-wasm_0.1.0_1764536647401_0.23517043752644495","host":"s3://npm-registry-packages-npm-production"}},"0.1.32":{"name":"ruvector-attention-wasm","type":"module","version":"0.1.32","description":"High-performance WebAssembly attention mechanisms for transformers and LLMs: Multi-Head, Flash Attention, Hyperbolic, Linear (Performer), MoE, Local-Global, and CGT Sheaf Attention with coherence gating. GPU-accelerated with SIMD fallback.","license":"MIT OR Apache-2.0","author":{"name":"RuVector Team","email":"team@ruvector.dev"},"repository":{"type":"git","url":"git+https://github.com/ruvnet/ruvector.git"},"homepage":"https://ruv.io/ruvector","bugs":{"url":"https://github.com/ruvnet/ruvector/issues"},"main":"ruvector_attention_wasm.js","module":"ruvector_attention_wasm.js","types":"ruvector_attention_wasm.d.ts","sideEffects":["./snippets/*"],"keywords":["wasm","webassembly","attention","transformer","llm","machine-learning","neural-networks","multi-head-attention","flash-attention","hyperbolic","moe","mixture-of-experts","coherence","cgt","sheaf-attention","ai","deep-learning","gpu","simd","infonce","contrastive-learning","performer","linear-attention"],"engines":{"node":">=16.0.0"},"publishConfig":{"access":"public"},"gitHead":"cc7b4a9ccc3d5e440df56379d3e93fd176e0ea5f","_id":"ruvector-attention-wasm@0.1.32","_nodeVersion":"25.3.0","_npmVersion":"11.6.2","dist":{"integrity":"sha512-2pt5dhcpHXXhNEVXeeDPzshXlGo3X8vWVuMfLA+fjUuXYFsW2YYrmUSgHOvEuqVxxow2Rgn9sZpxWQSBuT470g==","shasum":"4a1bcfe0530f2ba72248b91fcc3cbe132489f832","tarball":"https://registry.npmjs.org/ruvector-attention-wasm/-/ruvector-attention-wasm-0.1.32.tgz","fileCount":7,"unpackedSize":233082,"signatures":[{"keyid":"SHA256:DhQ8wR5APBvFHLF/+Tc+AYvPOdTpcIDqOhxsBHRwC7U","sig":"MEQCIEdN9jxXDSODQDisVh8C267PUcWfi3TcGemPEC1KIdVeAiA35JOnPEQ7a+uxBAgzDLIRQOYTUnIT/JWlmKoIWP5Sig=="}]},"_npmUser":{"name":"ruvnet","email":"ruv@ruv.net"},"directories":{},"maintainers":[{"name":"ruvnet","email":"ruv@ruv.net"}],"_npmOperationalInternal":{"host":"s3://npm-registry-packages-npm-production","tmp":"tmp/ruvector-attention-wasm_0.1.32_1769131215342_0.5567027196907617"},"_hasShrinkwrap":false}},"time":{"created":"2025-11-30T21:04:07.306Z","modified":"2026-01-23T01:20:15.644Z","0.1.0":"2025-11-30T21:04:07.611Z","0.1.32":"2026-01-23T01:20:15.530Z"},"bugs":{"url":"https://github.com/ruvnet/ruvector/issues"},"author":{"name":"RuVector Team","email":"team@ruvector.dev"},"license":"MIT OR Apache-2.0","homepage":"https://ruv.io/ruvector","keywords":["wasm","webassembly","attention","transformer","llm","machine-learning","neural-networks","multi-head-attention","flash-attention","hyperbolic","moe","mixture-of-experts","coherence","cgt","sheaf-attention","ai","deep-learning","gpu","simd","infonce","contrastive-learning","performer","linear-attention"],"repository":{"type":"git","url":"git+https://github.com/ruvnet/ruvector.git"},"description":"High-performance WebAssembly attention mechanisms for transformers and LLMs: Multi-Head, Flash Attention, Hyperbolic, Linear (Performer), MoE, Local-Global, and CGT Sheaf Attention with coherence gating. GPU-accelerated with SIMD fallback.","maintainers":[{"name":"ruvnet","email":"ruv@ruv.net"}],"readme":"# ruvector-attention-wasm\n\nWebAssembly bindings for the ruvector-attention package, providing high-performance attention mechanisms for browser and Node.js environments.\n\n## Features\n\n- **Multiple Attention Mechanisms**:\n  - Scaled Dot-Product Attention\n  - Multi-Head Attention\n  - Hyperbolic Attention (for hierarchical data)\n  - Linear Attention (Performer-style)\n  - Flash Attention (memory-efficient)\n  - Local-Global Attention\n  - Mixture of Experts (MoE) Attention\n  - **CGT Sheaf Attention** (coherence-gated via Prime-Radiant)\n\n- **Training Utilities**:\n  - InfoNCE contrastive loss\n  - Adam optimizer\n  - AdamW optimizer (with decoupled weight decay)\n  - Learning rate scheduler (warmup + cosine decay)\n\n- **TypeScript Support**: Full type definitions and modern API\n\n## Installation\n\n```bash\nnpm install ruvector-attention-wasm\n```\n\n## Usage\n\n### TypeScript/JavaScript\n\n```typescript\nimport { initialize, MultiHeadAttention, utils } from 'ruvector-attention-wasm';\n\n// Initialize WASM module\nawait initialize();\n\n// Create multi-head attention\nconst attention = new MultiHeadAttention({ dim: 64, numHeads: 8 });\n\n// Prepare inputs\nconst query = new Float32Array(64);\nconst keys = [new Float32Array(64), new Float32Array(64)];\nconst values = [new Float32Array(64), new Float32Array(64)];\n\n// Compute attention\nconst output = attention.compute(query, keys, values);\n\n// Use utilities\nconst similarity = utils.cosineSimilarity(query, keys[0]);\n```\n\n### Advanced Examples\n\n#### Hyperbolic Attention\n\n```typescript\nimport { HyperbolicAttention } from 'ruvector-attention-wasm';\n\nconst hyperbolic = new HyperbolicAttention({\n  dim: 128,\n  curvature: 1.0\n});\n\nconst output = hyperbolic.compute(query, keys, values);\n```\n\n#### MoE Attention with Expert Stats\n\n```typescript\nimport { MoEAttention } from 'ruvector-attention-wasm';\n\nconst moe = new MoEAttention({\n  dim: 64,\n  numExperts: 4,\n  topK: 2\n});\n\nconst output = moe.compute(query, keys, values);\n\n// Get expert utilization\nconst stats = moe.getExpertStats();\nconsole.log('Load balance:', stats.loadBalance);\n```\n\n#### Training with InfoNCE Loss\n\n```typescript\nimport { InfoNCELoss, Adam } from 'ruvector-attention-wasm';\n\nconst loss = new InfoNCELoss(0.07);\nconst optimizer = new Adam(paramCount, {\n  learningRate: 0.001,\n  beta1: 0.9,\n  beta2: 0.999,\n});\n\n// Training loop\nconst lossValue = loss.compute(anchor, positive, negatives);\noptimizer.step(params, gradients);\n```\n\n#### Learning Rate Scheduling\n\n```typescript\nimport { LRScheduler, AdamW } from 'ruvector-attention-wasm';\n\nconst scheduler = new LRScheduler({\n  initialLR: 0.001,\n  warmupSteps: 1000,\n  totalSteps: 10000,\n});\n\nconst optimizer = new AdamW(paramCount, {\n  learningRate: scheduler.getLR(),\n  weightDecay: 0.01,\n});\n\n// Training loop\nfor (let step = 0; step < 10000; step++) {\n  optimizer.learningRate = scheduler.getLR();\n  optimizer.step(params, gradients);\n  scheduler.step();\n}\n```\n\n## Building from Source\n\n### Prerequisites\n\n- Rust 1.70+\n- wasm-pack\n\n### Build Commands\n\n```bash\n# Build for web (ES modules)\nwasm-pack build --target web --out-dir pkg\n\n# Build for Node.js\nwasm-pack build --target nodejs --out-dir pkg-node\n\n# Build for bundlers (webpack, vite, etc.)\nwasm-pack build --target bundler --out-dir pkg-bundler\n\n# Run tests\nwasm-pack test --headless --firefox\n```\n\n## API Reference\n\n### Attention Mechanisms\n\n- `MultiHeadAttention` - Standard multi-head attention\n- `HyperbolicAttention` - Attention in hyperbolic space\n- `LinearAttention` - Linear complexity attention (Performer)\n- `FlashAttention` - Memory-efficient attention\n- `LocalGlobalAttention` - Combined local and global attention\n- `MoEAttention` - Mixture of Experts attention\n- `CGTSheafAttention` - Coherence-gated via Prime-Radiant energy\n- `scaledDotAttention()` - Functional API for basic attention\n\n### CGT Sheaf Attention (Prime-Radiant Integration)\n\nThe CGT (Coherence-Gated Transformer) Sheaf Attention mechanism uses Prime-Radiant's sheaf Laplacian energy to gate attention based on mathematical consistency:\n\n```typescript\nimport { CGTSheafAttention } from 'ruvector-attention-wasm';\n\nconst cgtAttention = new CGTSheafAttention({\n  dim: 128,\n  numHeads: 8,\n  coherenceThreshold: 0.3,  // Block if energy > threshold\n});\n\n// Attention is gated by coherence energy\nconst result = cgtAttention.compute(query, keys, values);\nconsole.log('Coherence energy:', result.energy);\nconsole.log('Is coherent:', result.isCoherent);\n```\n\n**Key features:**\n- Energy-weighted attention: Lower coherence energy â†’ higher attention\n- Automatic hallucination detection via residual analysis\n- GPU-accelerated with wgpu WGSL shaders (vec4 optimized)\n- SIMD fallback (AVX-512/AVX2/NEON)\n\n### Training\n\n- `InfoNCELoss` - Contrastive loss function\n- `Adam` - Adam optimizer\n- `AdamW` - AdamW optimizer with weight decay\n- `LRScheduler` - Learning rate scheduler\n\n### Utilities\n\n- `utils.cosineSimilarity()` - Cosine similarity between vectors\n- `utils.l2Norm()` - L2 norm of a vector\n- `utils.normalize()` - Normalize vector to unit length\n- `utils.softmax()` - Apply softmax transformation\n- `utils.attentionWeights()` - Compute attention weights from scores\n- `utils.batchNormalize()` - Batch normalization\n- `utils.randomOrthogonalMatrix()` - Generate random orthogonal matrix\n- `utils.pairwiseDistances()` - Compute pairwise distances\n\n## Performance\n\nThe WASM bindings provide near-native performance for attention computations:\n\n- Optimized with `opt-level = \"s\"` and LTO\n- SIMD acceleration where available\n- Efficient memory management\n- Zero-copy data transfer where possible\n\n## License\n\nMIT OR Apache-2.0\n","readmeFilename":"README.md"}